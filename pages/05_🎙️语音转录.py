import uuid

import streamlit as st
from st_audiorec import st_audiorec
from openai import OpenAI
from pydub import AudioSegment
from libs.msal import msal_auth
from libs import get_data_dir
import io
import sys
import os
from dotenv import load_dotenv

from libs.session import PageSessionState

sys.path.append(os.path.abspath('..'))
load_dotenv()

page_state = PageSessionState("speech")

# ç”¨äºå­˜å‚¨ä¸´æ—¶æ–‡ä»¶
data_dir = get_data_dir()

page_state.initn_attr("input_type", "microphone")
page_state.initn_attr("audio_text_source", None)
# è¯­éŸ³åˆæˆå†…å®¹
page_state.initn_attr("speech_recode", None)

st.sidebar.markdown("# ğŸ™ï¸è¯­éŸ³è½¬å½•ğŸ¤")

# åŸå§‹è¯­éŸ³æ–‡æœ¬ï¼Œè¯†åˆ«æˆ–è€…ä¸Šä¼ çš„å†…å®¹
content_box = st.empty()

if page_state.audio_text_source is not None:
    content_box.markdown(page_state.audio_text_source)
else:
    content_box.empty()


def on_flie_change():
    if page_state.uploaded_file is not None:
        stringio = io.StringIO(page_state.uploaded_file.getvalue().decode("utf-8"))
        string_data = stringio.read()
        page_state.audio_text_source = string_data
        content_box.markdown(string_data)


def clear_result():
    page_state.audio_text_source = None
    page_state.speech_recode = None
    content_box.empty()


if page_state.audio_text_source is None:
    wav_audio_recode = st_audiorec()
    if wav_audio_recode is not None:
        with st.spinner('æ­£åœ¨è¯†åˆ«è¯­éŸ³...'):
            audio_segment = AudioSegment.from_wav(io.BytesIO(wav_audio_recode))
            filename = os.path.join(data_dir, f"{uuid.uuid4()}.audio.wav")
            audio_segment.export(filename, format="wav")
            client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
            transcript = client.audio.transcriptions.create(
                model="whisper-1",
                response_format="json",
                file=open(filename, "rb"),
            )
            page_state.audio_text_source = transcript.text
            content_box.markdown(page_state.audio_text_source)

st.sidebar.file_uploader("ä¸Šä¼ æ–‡æœ¬æ–‡ä»¶", type=["txt", "md"],
                         on_change=on_flie_change, key="speech_uploaded_file")

st.sidebar.button("æ¸…ç©ºæ•°æ®", on_click=clear_result)

# æ˜¯å¦å·²ç»è¯†åˆ«è¯­éŸ³ä¿å­˜æ–‡æœ¬ç»“æœï¼Œ å¦‚æœæœ‰å°±å±•ç¤ºåˆæˆè¯­éŸ³ç•Œé¢éƒ¨åˆ†
if page_state.audio_text_source is not None:
    sound = st.selectbox("é€‰æ‹©éŸ³è‰²", ["alloy", "echo", "fable", "onyx", "nova", "shimmer"])
    c1, c2, c3 = st.columns(3)
    if c1.button("åˆæˆè¯­éŸ³"):
        client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        speech_file_path = os.path.join(data_dir, f"{uuid.uuid4()}.speech.mp3")
        with st.status("æ­£åœ¨åˆæˆè¯­éŸ³", expanded=True) as status:
            response = client.audio.speech.create(
                model="tts-1",
                voice=sound,
                input=page_state.audio_text_source
            )
            page_state.speech_recode = response.read()
            status.update(label="è¯­éŸ³åˆæˆå®Œæˆ!", state="complete")

    if page_state.speech_recode is not None:
        st.write(f"ğŸ§{sound}éŸ³è‰²")
        st.audio(page_state.speech_recode, format="audio/mp3")
        st.write(f"è¯­éŸ³{sound}åˆæˆå®Œæˆ")
        c3.download_button(
            label="ä¸‹è½½è¯­éŸ³",
            data=page_state.speech_recode,
            file_name='speech.mp3',
        )
